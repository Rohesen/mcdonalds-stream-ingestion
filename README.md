# ğŸ” Real-Time McDonald's Orders & Payments Streaming Pipeline

This project demonstrates how to simulate and stream real-time McDonald's order and payment data using **Apache Kafka**, **ksqlDB**, and **MongoDB Atlas**. It features a fully working data pipeline that supports real-time ingestion, stream joins, and analytical querying â€” visualized in MongoDB Dashboards.

---

## ğŸ› ï¸ Tech Stack

* **Apache Kafka** (Confluent Cloud)
* **ksqlDB**
* **MongoDB Atlas**
* **Python** (Mock data generation)
* **Avro** (Schema serialization)
* **MongoDB Charts**

---

## ğŸ“š Objective

The goal of this project is to:

* Simulate McDonaldâ€™s orders and payments in real-time
* Stream data using Kafka topics
* Join streams using `ksqlDB`
* Store enriched records in **MongoDB Atlas**
* Visualize trends via **MongoDB Dashboards**

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ kafka_producer.py                # Python script for mock data generation
â”œâ”€â”€ stream_definitions.sql           # ksqlDB stream creation and join logic
â”œâ”€â”€ Project_Screenshots/             # Architecture & setup screenshots
â”‚   â”œâ”€â”€ mcdonalds-streaming-rohesen.png
â”‚   â”œâ”€â”€ ss_order_stream.png
â”‚   â”œâ”€â”€ ss_payment_stream.png
â”‚   â”œâ”€â”€ ss_code_1.png
â”‚   â”œâ”€â”€ ss_code_2.png
â”‚   â”œâ”€â”€ ss_mongodb_cluster.png
â”‚   â””â”€â”€ mongodb_dashboard.png
â”œâ”€â”€ README.md                        # Project documentation
```

---

## ğŸ§± Architecture Overview

![Architecture Diagram](Project_Screenshots/mcdonalds-streaming-rohesen.png)

---

## ğŸ Python Producer

The mock data generator produces:

* 500 **orders**
* 500 **payments**
  Each order and payment pair is matched via a common `order_id`.

ğŸ“¸ Code Snippets:
![Producer Code Part 1](Project_Screenshots/ss_code_1.png)
![Producer Code Part 2](Project_Screenshots/ss_code_2.png)

---

## ğŸ”„ Kafka Topics

Two Avro-serialized topics were created in **Confluent Cloud**:

* `macd_orders`
* `macd_payments`

Each topic uses schemas stored in **Confluent Schema Registry**.

---

## ğŸ” Stream Processing via ksqlDB

Streams were defined in **ksqlDB** to process and enrich data:

### âœ… Order Stream

```sql
CREATE STREAM macd_orders_stream (...) WITH (...);
```

ğŸ“¸ Screenshot:
![Order Stream](Project_Screenshots/ss_order_stream.png)

### âœ… Payment Stream

```sql
CREATE STREAM macd_payments_stream (...) WITH (...);
```

ğŸ“¸ Screenshot:
![Payment Stream](Project_Screenshots/ss_payment_stream.png)

---

### ğŸ”— Stream Join

A joined stream `macd_orders_payments_joined` was created to correlate orders with their respective payments:

```sql
CREATE STREAM macd_orders_payments_joined AS
SELECT ...
FROM macd_orders_stream o
JOIN macd_payments_stream p
  WITHIN 24 HOURS
  ON o.order_id = p.order_id
EMIT CHANGES;
```

---

## ğŸ§© MongoDB Integration

MongoDB Atlas was used to store the joined output.

* Region: `ap-south-1` (same as Kafka for minimal latency)
* Collection: `orders_payments_joined`
* Data ingestion via **MongoDB Kafka Sink Connector**

ğŸ“¸ Screenshot of the Cluster:
![MongoDB Cluster](Project_Screenshots/ss_mongodb_cluster.png)

---

## ğŸ“ˆ MongoDB Dashboard

Once data was streamed into MongoDB, **MongoDB Charts** was used to build visualizations â€” like most popular items, payment methods, or hourly revenue.

ğŸ“¸ Dashboard Screenshot:
![MongoDB Chart](Project_Screenshots/mongodb_dashboard.png)

---

## ğŸ“ˆ Sample Use Cases

* Identify most frequently ordered items
* Monitor peak sales hours
* Compare revenue across payment methods
* Detect payment mismatches

---

## ğŸš€ Next Steps

* Add time-windowed aggregations (TUMBLING / HOPPING windows)
* Stream to other sinks like **ClickHouse**, **Postgres**, or **ElasticSearch**
* Alerting on anomalies using **Kafka Streams** or **ksqlDB alerts**
* Real-time frontend dashboard with **Streamlit** or **Grafana**

----
