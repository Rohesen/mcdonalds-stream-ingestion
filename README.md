# ğŸ” Real-Time McDonald's Orders & Payments Streaming Pipeline

This project demonstrates how to simulate and stream real-time McDonald's order and payment data using **Apache Kafka**, **ksqlDB**, and **MongoDB Atlas**. It features a fully working data pipeline that supports real-time ingestion, stream joins, and analytical querying â€” visualized in MongoDB Dashboards.

---

## ğŸ› ï¸ Tech Stack

- **Apache Kafka** (Confluent Cloud)
- **ksqlDB**
- **MongoDB Atlas**
- **Python** (Mock data generation)
- **Avro** (Schema serialization)
- **MongoDB Charts**

---

## ğŸ“š Objective

The goal of this project is to:

- Simulate McDonaldâ€™s orders and payments in real-time  
- Stream data using Kafka topics  
- Join streams using `ksqlDB`  
- Store enriched records in **MongoDB Atlas**  
- Visualize trends via **MongoDB Dashboards**

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ kafka_producer.py                # Python script for mock data generation
â”œâ”€â”€ stream_definitions.sql           # ksqlDB stream creation and join logic
â”œâ”€â”€ Project_Screenshots/             # Architecture & setup screenshots
â”‚   â”œâ”€â”€ mcdonalds-streaming-rohesen.png
â”‚   â”œâ”€â”€ ss_order_stream.png
â”‚   â”œâ”€â”€ ss_payment_stream.png
â”‚   â”œâ”€â”€ ss_code_1.png
â”‚   â”œâ”€â”€ ss_code_2.png
â”‚   â”œâ”€â”€ ss_mongodb_cluster.png
â”‚   â””â”€â”€ mongodb_dashboard.png
â”œâ”€â”€ README.md                        # Project documentation


## âœ… Steps Overview

### 1. ğŸ›  Kafka Setup

* Provisioned Kafka cluster using **Confluent Cloud** in `ap-south-1`.
* Enabled **Schema Registry** and created two topics:

  * `macd_orders`
  * `macd_payments`

---

### 2. ğŸ§ª Python Mock Data Generation

A Python script (`kafka_producer.py`) generates and publishes 500 mock orders and payments using Avro serialization.

#### ğŸ§¾ Sample Order Record

```json
{
  "order_id": "uuid",
  "customer_id": "cust_12345",
  "order_total": 52.30,
  "order_items": [
    {"item_name": "Big Mac", "quantity": 2, "price": 5.99}
  ],
  "order_time": 1721124935000
}
```

#### ğŸ§¾ Sample Payment Record

```json
{
  "payment_id": "uuid",
  "order_id": "same_as_order_id",
  "payment_amount": 52.30,
  "payment_method": "credit_card",
  "payment_time": 1721124960000
}
```

---

### 3. ğŸš€ ksqlDB Stream Processing

#### ğŸ”¹ Create Streams

```sql
CREATE STREAM macd_orders_stream (
  order_id STRING,
  ...
) WITH (
  KAFKA_TOPIC = 'macd_orders',
  ...
);
```

```sql
CREATE STREAM macd_payments_stream (
  payment_id STRING,
  ...
) WITH (
  KAFKA_TOPIC = 'macd_payments',
  ...
);
```

#### ğŸ”— Real-Time Join

```sql
CREATE STREAM macd_orders_payments_joined AS
SELECT ...
FROM macd_orders_stream o
INNER JOIN macd_payments_stream p
  WITHIN 24 HOURS
  ON o.order_id = p.order_id
EMIT CHANGES;
```

This produces a joined stream of orders + matching payments.

---

### 4. ğŸŒ MongoDB Integration

* Created a **MongoDB Atlas cluster** in `ap-south-1` (to match Kafka region).
* Integrated **ksqlDB Sink Connector** to stream `macd_orders_payments_joined` into MongoDB.
* Target Collection: `orders_payments_joined`

#### ğŸ§  Why MongoDB?

* Flexible schema for nested `order_items`
* Fast querying for BI dashboards
* Ideal for semi-structured analytical workloads

---

## ğŸ“Š Architecture

![Streaming Flow](assets/architecture.png)

---

## ğŸ§¹ Next Steps

* Add windowed aggregations (e.g., hourly revenue)
* Visualize in a dashboard (e.g., Grafana / MongoDB Charts)
* Add alerting (e.g., payment mismatches)

---

## ğŸ“š Learnings

* âœ… Kafka Streams via Python (Confluent)
* âœ… Schema evolution with Avro
* âœ… Real-time joins in ksqlDB with `WITHIN 24 HOURS`
* âœ… Region-matching for Confluent â†” MongoDB connectivity

---

## ğŸ“¬ Contact

Have questions? Connect with me on [LinkedIn](#) or raise an issue here.

```

---

Let me know if you'd like me to generate the `architecture.png` file content or help you push this project to GitHub with a license, `.gitignore`, etc.
```
